{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from ae_ddpm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class EncoderM(BaseEncoder):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channel: StrictInt,\n",
    "            channel: StrictInt,\n",
    "            channel_multiplier: List[StrictInt],\n",
    "            n_res_blocks: StrictInt,\n",
    "            attn_strides: List[StrictInt],\n",
    "            attn_heads: StrictInt = 1,\n",
    "            dropout: StrictFloat = 0,\n",
    "            fold: StrictInt = 1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_channel,\n",
    "            channel,\n",
    "            channel_multiplier,\n",
    "            n_res_blocks,\n",
    "            attn_strides,\n",
    "            attn_heads,\n",
    "            dropout,\n",
    "            fold)\n",
    "        group_norm = channel // 4\n",
    "        in_channel = channel * 4\n",
    "        self.mid = nn.ModuleList(\n",
    "            [\n",
    "                EncResBlockWithAttention(\n",
    "                    in_channel,\n",
    "                    in_channel,\n",
    "                    dropout=dropout,\n",
    "                    use_attention=True,\n",
    "                    attention_head=attn_heads,\n",
    "                    group_norm=group_norm\n",
    "                ),\n",
    "                EncResBlockWithAttention(\n",
    "                    in_channel,\n",
    "                    in_channel,\n",
    "                    dropout=dropout,\n",
    "                    group_norm=group_norm\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(channel * 4 * 8 * 8, 512)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = super().forward(input)\n",
    "        print(x.shape)\n",
    "        for layer in self.mid:\n",
    "            x = layer(x)\n",
    "        x = self.out(x.flatten(start_dim=1))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "enc = EncoderM(in_channel=3,\n",
    "                      channel=128,\n",
    "                      channel_multiplier=[1, 2, 2, 4, 4],\n",
    "                      n_res_blocks=2,\n",
    "                      attn_strides=[8, 16],\n",
    "                      attn_heads=4,\n",
    "                      dropout=0,\n",
    "                      fold=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 1.7648e-01,  8.5962e-02,  3.8631e-02, -1.2128e-01,  3.5422e-02,\n         -1.1440e-01,  1.4193e-01, -6.0567e-02, -1.1883e-01,  8.9824e-02,\n         -6.4125e-02, -1.2256e-01, -7.8203e-02, -9.7997e-02, -1.0199e-01,\n         -4.2376e-03,  2.7806e-02,  2.3010e-02, -1.4266e-02, -3.1796e-02,\n         -2.1724e-02, -6.4066e-02,  1.2855e-02, -5.6851e-02, -7.0995e-02,\n         -8.8030e-03, -1.7191e-01,  1.3001e-01,  4.5608e-02, -8.1250e-02,\n          2.4362e-03, -1.3606e-01, -3.4377e-02,  1.2428e-01,  3.4799e-02,\n          5.4412e-02,  1.1654e-01,  1.0832e-01,  4.9515e-02, -2.9227e-02,\n         -1.4401e-02, -6.5456e-02,  9.4891e-02,  6.0969e-02,  5.7815e-02,\n          3.5636e-02, -1.1265e-02,  5.3637e-02, -1.2299e-01, -7.7614e-02,\n          2.8612e-02,  4.0059e-03,  2.0339e-01, -1.4915e-02, -1.0014e-01,\n         -3.4283e-02, -2.1381e-01, -1.2323e-01,  3.2159e-02, -1.2821e-02,\n          8.5107e-02, -4.6876e-02, -2.7548e-02,  3.0883e-02,  5.4227e-02,\n         -1.2840e-02, -1.6232e-01,  1.1021e-02,  8.8294e-02,  4.6989e-02,\n          1.5471e-01,  6.3126e-02,  1.1456e-03, -1.5943e-01,  2.6350e-02,\n          9.3401e-02,  1.4796e-01,  8.6263e-04, -3.4175e-03, -2.6752e-02,\n         -5.1075e-02, -1.1767e-01,  9.4802e-02, -6.3633e-02, -1.2092e-01,\n         -1.5073e-01,  7.2361e-02, -1.0260e-02,  1.4179e-01,  7.5707e-02,\n         -6.2400e-02, -6.6916e-03, -4.0954e-03,  1.6562e-02, -6.9801e-03,\n         -2.2152e-02,  2.4748e-02, -3.9808e-03,  6.5786e-03,  2.1394e-02,\n         -1.1501e-01, -3.9975e-02, -2.0285e-01, -5.2208e-02,  6.1570e-02,\n         -4.6629e-03,  8.2504e-02,  1.1251e-01, -9.5560e-04,  4.7118e-02,\n         -5.6083e-02, -9.7852e-02, -9.8503e-02, -4.6628e-02, -3.3357e-02,\n         -6.3411e-02,  3.8183e-02, -7.3655e-03,  2.1067e-02,  4.1794e-02,\n          1.7589e-02, -1.2292e-01, -1.8422e-02, -8.4472e-02,  1.0501e-01,\n         -4.7255e-02, -9.1367e-02, -2.2773e-02,  8.7860e-02,  9.6433e-02,\n          3.8900e-02, -5.0659e-02, -1.6764e-02,  1.3542e-01, -5.1277e-02,\n         -1.0553e-02,  6.2679e-02, -1.8525e-02, -3.0720e-02, -3.7836e-02,\n         -2.1092e-03,  5.8183e-02, -7.5270e-02,  3.2489e-02,  1.3205e-02,\n          8.9073e-02, -5.2310e-02, -7.3998e-02, -9.4248e-02,  1.1200e-01,\n         -8.5603e-02,  5.0989e-02,  4.0957e-02, -1.3293e-01,  1.2302e-03,\n         -4.3878e-02, -1.0797e-01, -9.4568e-02, -1.2275e-01, -1.2570e-01,\n         -5.6175e-02,  3.5378e-02, -3.5669e-03, -6.8211e-02,  1.7417e-01,\n         -9.3508e-02, -5.5007e-02, -6.7117e-02,  5.1238e-02, -1.7706e-02,\n          5.4814e-02, -3.3408e-02,  4.3448e-03,  2.7666e-02,  1.1199e-02,\n         -9.2419e-02,  2.8070e-02, -2.6143e-02, -3.7580e-02,  3.5911e-02,\n         -1.6236e-02, -8.8488e-02, -6.4528e-02, -1.2922e-03, -9.2081e-02,\n          1.7028e-02,  1.9279e-02,  7.8104e-02, -1.3230e-01,  9.3371e-02,\n          9.3809e-02, -7.7056e-02, -2.7726e-02,  1.4625e-02, -1.0707e-01,\n         -3.2335e-02, -3.2713e-03,  1.0317e-01,  1.9425e-02, -7.3168e-02,\n         -1.2887e-01,  1.5404e-01,  1.2285e-01, -3.2777e-02,  9.7971e-02,\n         -2.7907e-01, -5.3789e-02, -9.4275e-02, -2.0825e-02, -1.8158e-02,\n         -1.3601e-01,  2.1123e-02,  3.2987e-03, -1.3261e-02,  1.0252e-01,\n          5.0732e-02, -1.6217e-01,  2.0488e-02, -4.3651e-02,  8.3306e-03,\n          4.7333e-02,  8.0177e-02, -8.6660e-02,  1.2850e-01,  1.7151e-01,\n         -7.8331e-03,  5.7769e-02, -2.4203e-02,  1.8197e-02, -2.3503e-01,\n         -3.3979e-02,  3.7089e-02,  1.6725e-01, -9.6070e-02, -4.2699e-02,\n         -9.7023e-02,  3.2782e-02, -3.7242e-02,  1.0014e-01, -1.6658e-01,\n          3.1929e-02, -3.0898e-03, -7.5401e-02, -5.7694e-02, -1.7590e-02,\n         -7.2293e-02,  5.1455e-02,  4.4183e-02, -1.0114e-01, -1.6523e-02,\n          1.1112e-01,  1.4170e-01,  6.9669e-02,  3.5614e-02, -1.6361e-02,\n          2.7229e-02,  7.8685e-02, -5.8235e-02,  4.1136e-02,  2.5508e-02,\n          1.1893e-01, -7.8833e-02,  9.1629e-02,  8.4744e-02,  4.6682e-02,\n          5.9520e-02,  5.6644e-02, -1.0054e-01, -6.0755e-02,  8.6367e-02,\n          6.6606e-02, -4.5179e-02,  2.1434e-02,  2.4538e-02, -4.7586e-02,\n         -1.0833e-01,  7.1025e-02, -4.7544e-02, -4.9446e-02,  1.2861e-01,\n          2.6210e-02, -8.9625e-02, -6.6245e-02, -1.1051e-01, -8.3179e-02,\n          1.4110e-02,  7.6937e-02,  1.3251e-01,  1.0843e-01, -9.5799e-04,\n          8.5093e-02, -3.6015e-02, -1.6319e-02,  1.8891e-02, -1.1660e-02,\n         -1.9087e-02,  1.3028e-02, -6.1501e-02,  9.7170e-02,  1.4618e-01,\n         -2.6964e-02, -3.0233e-02,  5.9007e-02,  7.2641e-03, -2.5041e-02,\n         -5.0741e-02, -1.2834e-02,  1.2901e-02,  1.1923e-02, -4.6189e-02,\n          3.7210e-02,  7.5757e-02, -5.6799e-02, -1.7513e-02,  1.0479e-01,\n          1.8773e-02,  1.1069e-01,  9.3342e-02,  8.5128e-03, -6.2694e-02,\n         -1.5394e-01, -1.1976e-02,  3.4022e-02, -1.6332e-01,  6.9575e-02,\n         -1.4477e-02, -5.2248e-02, -4.2072e-02, -5.6221e-02,  7.9245e-02,\n         -6.1399e-02, -2.6731e-03, -3.5454e-02,  1.4755e-02, -2.9285e-02,\n          2.1702e-02,  4.1532e-03, -7.8622e-03, -6.7907e-02,  6.1682e-02,\n         -3.4355e-02, -4.0702e-02, -4.3973e-02,  5.9925e-03, -4.5607e-02,\n          8.5669e-02, -5.8627e-02,  7.7071e-02,  5.8042e-02, -7.1026e-02,\n         -5.2615e-02,  6.2858e-02, -4.8585e-02, -9.1242e-02, -2.6016e-02,\n          2.3242e-02, -4.2659e-02, -9.9428e-02,  1.0563e-01, -6.8816e-04,\n         -4.3252e-03, -1.1067e-02, -4.3164e-02, -1.5925e-02, -1.7146e-01,\n         -1.8790e-01, -1.0248e-01,  3.8284e-02, -1.1255e-01,  1.4262e-01,\n         -7.4511e-02, -6.3406e-04, -1.1032e-01, -2.5757e-02, -2.3259e-02,\n          5.0152e-02, -2.9205e-02, -4.4580e-02,  4.5159e-02, -4.4292e-02,\n          6.3505e-03,  1.4558e-02, -1.3812e-02, -9.2558e-02, -2.8043e-02,\n         -4.3763e-02, -4.5153e-02,  7.5283e-02, -9.2569e-02, -4.0393e-03,\n          2.4460e-02, -1.6167e-01,  6.5123e-02, -2.0302e-01, -7.9929e-02,\n         -1.0885e-01,  1.6060e-02, -3.9932e-03,  1.0661e-01, -4.6032e-02,\n         -6.8542e-02, -5.8751e-02,  9.8558e-02, -6.6369e-02,  1.2962e-03,\n          2.4311e-02, -2.7009e-01,  5.9317e-02,  6.7696e-03, -7.9548e-03,\n         -5.5471e-02,  1.1253e-01, -1.1273e-01,  2.0980e-02,  1.5890e-01,\n          2.6932e-02, -1.3270e-01, -3.9286e-03,  5.1444e-02, -3.2242e-02,\n          1.7367e-02, -1.6654e-04,  2.3282e-02, -1.7733e-01, -3.3509e-02,\n          5.1338e-02, -6.9798e-02,  3.1497e-02, -9.0008e-02, -1.3353e-01,\n          8.6423e-02,  9.2685e-02, -5.2813e-02, -6.4229e-02,  7.2594e-02,\n         -9.2843e-02,  2.0756e-02,  3.3417e-02,  7.8880e-02, -3.2865e-02,\n          8.1244e-02,  7.7496e-02, -5.4079e-02,  3.1907e-02, -3.0004e-02,\n         -3.5652e-02, -1.4748e-01, -6.0287e-02, -4.7525e-02,  7.5846e-02,\n         -1.4748e-02, -3.5174e-02, -4.2325e-02, -1.3606e-02, -1.9236e-03,\n          9.1474e-02,  1.0859e-01,  2.1455e-02, -1.8621e-02, -3.9194e-02,\n          4.0788e-02, -6.3720e-02,  1.4099e-02,  1.5065e-01,  8.8908e-03,\n          7.3854e-02,  4.6412e-03,  8.6038e-03, -6.0181e-02,  1.0810e-01,\n          6.7065e-02,  4.7873e-02, -7.0310e-03, -5.9264e-02,  2.7034e-02,\n         -2.0273e-02,  1.6119e-01, -4.6662e-02, -5.2886e-02,  5.2178e-02,\n          3.1873e-02,  1.5057e-01, -5.1439e-02, -5.9896e-02,  1.0758e-01,\n         -6.8301e-02, -2.7616e-02, -6.0016e-02, -3.0257e-02,  1.3588e-01,\n          7.5645e-02, -1.3946e-02, -3.2521e-02, -7.9958e-03, -1.5284e-01,\n          1.2501e-01, -1.6399e-02,  5.4258e-02,  1.8546e-01, -1.0322e-01,\n          7.0485e-02, -5.9539e-02, -2.4200e-02,  4.7588e-02, -1.6669e-02,\n          7.1430e-02, -4.0345e-02,  7.9837e-02,  7.6411e-02,  1.2079e-01,\n          7.1399e-04,  1.3408e-01]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(torch.randn(1, 3, 128, 128))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from vit_pytorch.twins_svt import TwinsSVT\n",
    "\n",
    "encoder = TwinsSVT(\n",
    "    num_classes=512,  # number of output classes\n",
    "    s1_emb_dim=64,  # stage 1 - patch embedding projected dimension\n",
    "    s1_patch_size=4,  # stage 1 - patch size for patch embedding\n",
    "    s1_local_patch_size=7,  # stage 1 - patch size for local attention\n",
    "    s1_global_k=7,  # stage 1 - global attention key / value reduction factor, defaults to 7 as specified in paper\n",
    "    s1_depth=1,  # stage 1 - number of transformer blocks (local attn -> ff -> global attn -> ff)\n",
    "    s2_emb_dim=128,  # stage 2 (same as above)\n",
    "    s2_patch_size=2,\n",
    "    s2_local_patch_size=7,\n",
    "    s2_global_k=7,\n",
    "    s2_depth=1,\n",
    "    s3_emb_dim=256,  # stage 3 (same as above)\n",
    "    s3_patch_size=2,\n",
    "    s3_local_patch_size=7,\n",
    "    s3_global_k=7,\n",
    "    s3_depth=5,\n",
    "    s4_emb_dim=512,  # stage 4 (same as above)\n",
    "    s4_patch_size=2,\n",
    "    s4_local_patch_size=7,\n",
    "    s4_global_k=7,\n",
    "    s4_depth=4,\n",
    "    peg_kernel_size=3,  # positional encoding generator kernel size\n",
    "    dropout=0.  # dropout\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b c (x p1) (y p2) -> (b x y) c p1 p2\".\n Input tensor shape: torch.Size([1, 64, 32, 32]). Additional info: {'p1': 7, 'p2': 7}.\n Shape mismatch, can't divide axis of length 32 in chunks of 7",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mEinopsError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/einops/einops.py\u001B[0m in \u001B[0;36mreduce\u001B[0;34m(tensor, pattern, reduction, **axes_lengths)\u001B[0m\n\u001B[1;32m    408\u001B[0m         \u001B[0mrecipe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_prepare_transformation_recipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpattern\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxes_lengths\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhashable_axes_lengths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 409\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_apply_recipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecipe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreduction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    410\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mEinopsError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/einops/einops.py\u001B[0m in \u001B[0;36m_apply_recipe\u001B[0;34m(recipe, tensor, reduction_type)\u001B[0m\n\u001B[1;32m    231\u001B[0m     \u001B[0minit_shapes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduced_axes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxes_reordering\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madded_axes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfinal_shapes\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 232\u001B[0;31m         \u001B[0m_reconstruct_from_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecipe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbackend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    233\u001B[0m     \u001B[0mtensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbackend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minit_shapes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/einops/einops.py\u001B[0m in \u001B[0;36m_reconstruct_from_shape_uncached\u001B[0;34m(self, shape)\u001B[0m\n\u001B[1;32m    196\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlength\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mknown_product\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlength\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mknown_product\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 197\u001B[0;31m                     raise EinopsError(\"Shape mismatch, can't divide axis of length {} in chunks of {}\".format(\n\u001B[0m\u001B[1;32m    198\u001B[0m                         length, known_product))\n",
      "\u001B[0;31mEinopsError\u001B[0m: Shape mismatch, can't divide axis of length 32 in chunks of 7",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mEinopsError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_4469/3904200563.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mencoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m128\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m128\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/vit_pytorch/twins_svt.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    227\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 229\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/vit_pytorch/twins_svt.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    164\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mlocal_attn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mff1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobal_attn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mff2\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 166\u001B[0;31m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlocal_attn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    167\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mff1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    168\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mglobal_attn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/vit_pytorch/twins_svt.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, **kwargs)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 31\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     32\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mLayerNorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mModule\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/vit_pytorch/twins_svt.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, **kwargs)\u001B[0m\n\u001B[1;32m     51\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 53\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     54\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mFeedForward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mModule\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/vit_pytorch/twins_svt.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, fmap)\u001B[0m\n\u001B[1;32m    108\u001B[0m         \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mt\u001B[0m \u001B[0;34m//\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0mfmap\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrearrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfmap\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'b c (x p1) (y p2) -> (b x y) c p1 p2'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m         \u001B[0mq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_q\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfmap\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_kv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfmap\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/einops/einops.py\u001B[0m in \u001B[0;36mrearrange\u001B[0;34m(tensor, pattern, **axes_lengths)\u001B[0m\n\u001B[1;32m    484\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Rearrange can't be applied to an empty list\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    485\u001B[0m         \u001B[0mtensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstack_on_zeroth_dimension\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 486\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpattern\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'rearrange'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0maxes_lengths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    487\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    488\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ML/VENV21/lib/python3.9/site-packages/einops/einops.py\u001B[0m in \u001B[0;36mreduce\u001B[0;34m(tensor, pattern, reduction, **axes_lengths)\u001B[0m\n\u001B[1;32m    415\u001B[0m             \u001B[0mmessage\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m'\\n Input is list. '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    416\u001B[0m         \u001B[0mmessage\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m'Additional info: {}.'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxes_lengths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 417\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mEinopsError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'\\n {}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    418\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    419\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mEinopsError\u001B[0m:  Error while processing rearrange-reduction pattern \"b c (x p1) (y p2) -> (b x y) c p1 p2\".\n Input tensor shape: torch.Size([1, 64, 32, 32]). Additional info: {'p1': 7, 'p2': 7}.\n Shape mismatch, can't divide axis of length 32 in chunks of 7"
     ]
    }
   ],
   "source": [
    "encoder(torch.randn(1, 3, 128, 128))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128%28"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "144"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128+16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from nystrom_attention import Nystromformer\n",
    "from einops import repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class EncoderV3(BaseEncoder):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channel: StrictInt,\n",
    "            channel: StrictInt,\n",
    "            channel_multiplier: List[StrictInt],\n",
    "            n_res_blocks: StrictInt,\n",
    "            attn_strides: List[StrictInt],\n",
    "            attn_heads: StrictInt = 1,\n",
    "            dropout: StrictFloat = 0,\n",
    "            fold: StrictInt = 1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_channel,\n",
    "            channel,\n",
    "            channel_multiplier,\n",
    "            n_res_blocks,\n",
    "            attn_strides,\n",
    "            attn_heads,\n",
    "            dropout,\n",
    "            fold)\n",
    "        group_norm = channel // 4\n",
    "        self.mid = nn.ModuleList(\n",
    "            [\n",
    "                EncResBlockWithAttention(\n",
    "                    in_channel,\n",
    "                    in_channel,\n",
    "                    dropout=dropout,\n",
    "                    use_attention=True,\n",
    "                    attention_head=attn_heads,\n",
    "                    group_norm=group_norm\n",
    "                ),\n",
    "                EncResBlockWithAttention(\n",
    "                    in_channel,\n",
    "                    in_channel,\n",
    "                    dropout=dropout,\n",
    "                    group_norm=group_norm\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #self.out = nn.Linear(channel * 4 * 8 * 8, 512)\n",
    "        self.skip = nn.Linear(channel * 4 * 8 * 8, 512)\n",
    "\n",
    "        t_dim = channel * 4\n",
    "        t_patches = 8 * 8\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, t_patches + 1, t_dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, t_dim))\n",
    "\n",
    "        self.transformer = Nystromformer(\n",
    "            dim = t_dim,\n",
    "            depth = 8,\n",
    "            heads = 4,\n",
    "            num_landmarks = 256\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = super().forward(input)\n",
    "        skip = self.skip(x.flatten(start_dim=1))\n",
    "        #x = x.flatten(start_dim=2)\n",
    "        #x = self.to_patch_embedding(x)\n",
    "        #print(\"ggg\", x.shape)\n",
    "        x = x.flatten(start_dim=2).permute(0, 2, 1)\n",
    "\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        #x = self.dropout(x)\n",
    "        x = self.transformer(x)[:, 0]\n",
    "        out = x + skip\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "encoder = EncoderV3(in_channel=3,\n",
    "                  channel=128,\n",
    "                  channel_multiplier=[1, 2, 2, 4, 4],\n",
    "                  n_res_blocks=2,\n",
    "                  attn_strides=[8, 16],\n",
    "                  attn_heads=4,\n",
    "                  dropout=0,\n",
    "                  fold=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggg torch.Size([5, 512]) torch.Size([5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 0.6818, -0.6286, -1.2552,  ...,  3.6689, -0.7544,  2.2791],\n        [ 0.6940, -0.4346, -1.0720,  ...,  3.7116, -0.9664,  2.2288],\n        [ 0.7017, -0.5301, -1.2037,  ...,  3.7276, -0.9122,  2.2274],\n        [ 0.6755, -0.4856, -1.2206,  ...,  3.6690, -0.7638,  2.1814],\n        [ 0.6901, -0.5135, -1.2251,  ...,  3.6426, -0.8047,  2.1940]],\n       grad_fn=<AddBackward0>)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(torch.randn(5, 3, 128, 128))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}